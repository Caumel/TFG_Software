{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1727,
     "status": "ok",
     "timestamp": 1550100164594,
     "user": {
      "displayName": "Luis Caumel",
      "photoUrl": "",
      "userId": "08450739378384452972"
     },
     "user_tz": -60
    },
    "id": "5UhweCKcBoOo",
    "outputId": "944cd4bb-c8af-47b9-aeb4-b24f41019cf9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import,division,print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from PIL import Image as img\n",
    "import cv2\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Lambda, Flatten, Activation\n",
    "from keras import backend as K\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aH1ZRvQPDtCc"
   },
   "outputs": [],
   "source": [
    "def abs_diff_output_shape(input_shapes):\n",
    "  shape1, shape2, shape3= input_shapes\n",
    "  return shape1\n",
    "\n",
    "\n",
    "def distance_vec(y_pred): #Cosine distance\n",
    "  x = y_pred[0]\n",
    "  y = y_pred[1]\n",
    "  z = y_pred[2]\n",
    "  negative = K.prod(K.stack([x, y], axis=1), axis=1)\n",
    "  return negative\n",
    "  \n",
    "  \n",
    "  anchor = y_pred[0]\n",
    "  positive = y_pred[1]\n",
    "  negative = y_pred[2]\n",
    "\n",
    "def get_abs_diff(y_pred):\n",
    "    # L1 distance between two vectors\n",
    "    x = y_pred[0]\n",
    "    y = y_pred[1]\n",
    "    z = y_pred[2]\n",
    "    return K.abs(x - y)\n",
    "\n",
    "\n",
    "def triple_loss(y_true, y_pred):   # y_pred contiene la imagen ancla, positiva y negativa\n",
    "  alpha = 0.1\n",
    "  \n",
    "  anchor = y_pred[0]\n",
    "  positive = y_pred[1]\n",
    "  negative = y_pred[2]\n",
    "    \n",
    "  positive_distance = K.mean(K.square(anchor-positive),axis=-1)  #En principio mean y sum hacen lo mismo excepto que una hace media y otro suma\n",
    "  negative_distance = K.mean(K.square(anchor-negative),axis=-1)\n",
    "  \n",
    "  loss = positive_distance - negative_distance\n",
    "  return K.maximum(loss + alpha, 0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jlov2ltiHFGo"
   },
   "outputs": [],
   "source": [
    "def get_random_image(img_groups, group_names, gid):\n",
    "    gname = group_names[gid]\n",
    "    photos = img_groups[gname]\n",
    "    pid = np.random.choice(np.arange(len(photos)), size=1)[0]\n",
    "    pname = photos[pid]\n",
    "    return gname + pname + \".jpg\"\n",
    "   \n",
    "def create_triplet(number_images,minImage,maxImage): # Crea tripleta\n",
    "  img_groups = {}\n",
    "  for folder in  sorted(os.listdir('/home/luis/dataset'),reverse=False):\n",
    "    i = 0 \n",
    "    for img_file in sorted(os.listdir('/home/luis/dataset/' + folder),reverse=False):\n",
    "      #if i == number_images: # Para solo coger 40 fotos\n",
    "        #break\n",
    "      if i < minImage:\n",
    "        i = i + 1\n",
    "        continue\n",
    "      if i > maxImage:\n",
    "        break\n",
    "      prefix, suffix = img_file.split(\".\")\n",
    "      pid = prefix[3:]\n",
    "      if folder in img_groups:\n",
    "          img_groups[folder].append(pid)\n",
    "      else:\n",
    "          img_groups[folder] = [pid]\n",
    "      i += 1\n",
    "  pos_triples, neg_triples, labels = [], [], []\n",
    "  # positive pairs and negative \n",
    "  group_names = list(img_groups.keys())\n",
    "  for key in img_groups.keys():\n",
    "    for i in range(0,number_images):\n",
    "      for j in range(i+1,number_images): # Si dejo i me cogeria duplas con elementos iguales osea (001,001)\n",
    "        inc = random.randrange(1, classes+1)\n",
    "        dn = (int(key) + inc) % classes\n",
    "        right = get_random_image(img_groups, group_names, dn)\n",
    "        pos_triples.append((key + img_groups[key][i] + \".jpg\", key + img_groups[key][j] + \".jpg\", right))\n",
    "  return pos_triples  \n",
    "\n",
    "def load_image(image_name):\n",
    "    image = cv2.imread('/home/luis/dataset/' + image_name[0:3] + '/' + image_name)\n",
    "    image = cv2.resize(image, (96, 96))\n",
    "    image = img_to_array(image)\n",
    "    return image     \n",
    "           \n",
    "def triples_batch(tamaño,image_triples):#, batch_size): #Tripletas\n",
    "    images = np.empty([tamaño,3,96,96,3])\n",
    "    for index,i in enumerate(image_triples):\n",
    "        ahs, phs, nhs= i\n",
    "        a = (load_image(ahs),load_image(phs),load_image(nhs))\n",
    "        images[index] = a\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_image(img_groups, group_names, gid):     #Este metodo es para juntar todas con todas\n",
    "    gname = group_names[gid]\n",
    "    photos = img_groups[gname]\n",
    "    pid = np.random.choice(np.arange(len(photos)), size=1)[0]\n",
    "    pname = photos[pid]\n",
    "    return gname + pname + \".jpg\"\n",
    "   \n",
    "def create_triplet(number_images,minImage,maxImage): # Crea tripleta\n",
    "  img_groups = {}\n",
    "  for folder in  sorted(os.listdir('/home/luis/dataset'),reverse=False):\n",
    "    i = 0 \n",
    "    for img_file in sorted(os.listdir('/home/luis/dataset/' + folder),reverse=False):\n",
    "      #if i == number_images: # Para solo coger 40 fotos\n",
    "        #break\n",
    "      if i < minImage:\n",
    "        i = i + 1\n",
    "        continue\n",
    "      if i > maxImage:\n",
    "        break\n",
    "      print(img_file)\n",
    "      prefix, suffix = img_file.split(\".\")\n",
    "      pid = prefix[3:]\n",
    "      if folder in img_groups:\n",
    "          img_groups[folder].append(pid)\n",
    "      else:\n",
    "          img_groups[folder] = [pid]\n",
    "      i += 1\n",
    "  pos_triples = []\n",
    "  # positive pairs and negative \n",
    "  group_names = list(img_groups.keys())\n",
    "  for key in img_groups.keys():\n",
    "    for i in range(0,number_images):\n",
    "        for j in range(i+1,number_images): # Si dejo i me cogeria duplas con elementos iguales osea (001,001)\n",
    "            for k in img_groups.keys():\n",
    "                if k != key:\n",
    "                    for l in range(0,number_images):\n",
    "                        print((key + img_groups[key][i] + \".jpg\", key + img_groups[key][j] + \".jpg\", k + img_groups[k][l]))\n",
    "                        pos_triples.append((key + img_groups[key][i] + \".jpg\", key + img_groups[key][j] + \".jpg\", k + img_groups[k][l]))\n",
    "  return pos_triples\n",
    "\n",
    "def load_image(image_name):\n",
    "    image = cv2.imread('/home/luis/dataset/' + image_name[0:3] + '/' + image_name)\n",
    "    image = cv2.resize(image, (96, 96))\n",
    "    image = img_to_array(image)\n",
    "    return image     \n",
    "           \n",
    "def triples_batch(tamaño,image_triples):#, batch_size): #Tripletas\n",
    "    images = np.empty([tamaño,3,96,96,3])\n",
    "    for index,i in enumerate(image_triples):\n",
    "        ahs, phs, nhs= i\n",
    "        a = (load_image(ahs),load_image(phs),load_image(nhs))\n",
    "        images[index] = a\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KlIPAdDyHRcz"
   },
   "outputs": [],
   "source": [
    "def create_base_network(input_dim, classes): # SmallerVGGnet\n",
    "  red = Sequential() \n",
    "  red.add(Conv2D(32, (3, 3), padding=\"same\",input_shape=input_dim))\n",
    "  red.add(Activation(\"relu\"))\n",
    "  red.add(BatchNormalization(axis=-1))\n",
    "  red.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "  #red.add(Dropout(0.25))\n",
    "  \n",
    "  # (CONV => RELU) * 2 => POOL\n",
    "  red.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "  red.add(Activation(\"relu\"))\n",
    "  red.add(BatchNormalization(axis=-1))\n",
    "  red.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "  red.add(Activation(\"relu\"))\n",
    "  red.add(BatchNormalization(axis=-1))\n",
    "  red.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  #red.add(Dropout(0.25))\n",
    "  \n",
    "  # (CONV => RELU) * 2 => POOL\n",
    "  red.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "  red.add(Activation(\"relu\"))\n",
    "  red.add(BatchNormalization(axis=-1))\n",
    "  red.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "  red.add(Activation(\"relu\"))\n",
    "  red.add(BatchNormalization(axis=-1))\n",
    "  red.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  #red.add(Dropout(0.25))\n",
    "  \n",
    "\t# first (and only) set of FC => RELU layers\n",
    "  red.add(Flatten())\n",
    "  #red.add(Dense(2048))\n",
    "  #red.add(Activation(\"relu\"))\n",
    "  #red.add(BatchNormalization())\n",
    "  #red.add(Dropout(0.5))\n",
    "\n",
    "  red.add(Dense(1024))\n",
    "  red.add(Activation(\"relu\"))\n",
    "  red.add(BatchNormalization())\n",
    "  red.add(Dropout(0.5))\n",
    "\n",
    "  return red "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 471,
     "status": "ok",
     "timestamp": 1550100187214,
     "user": {
      "displayName": "Luis Caumel",
      "photoUrl": "",
      "userId": "08450739378384452972"
     },
     "user_tz": -60
    },
    "id": "DbZ40Cjvp5HQ",
    "outputId": "ae4cc33e-7155-40a9-d5e8-6b8abc80d469"
   },
   "outputs": [],
   "source": [
    "number_images = 4 # Numero de imagenes cogidas\n",
    "classes = 151 # Numero de pokemones\n",
    "batch = 32\n",
    "nb_epoch = 10\n",
    "\n",
    "input_dim = (96,96,3) # Son el ancho el alto y la profundidad de la imagen, 3 al ser en color es la variable input_dim pero todavia no esta tocada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGB199xDq8bn",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# image triples: 906\n",
      "# image triples: 800\n",
      "# image triples: 106\n"
     ]
    }
   ],
   "source": [
    "triples_data = create_triplet(number_images,0,3)\n",
    "\n",
    "print(\"# image triples:\", len(triples_data))\n",
    "\n",
    "tm_train = 800\n",
    "tm_val = 906\n",
    "\n",
    "tr_train = triples_data[0:tm_train]\n",
    "tr_val = triples_data[tm_train:tm_val]  #El tamaño tiene que ser par (No se mucho por que)\n",
    "\n",
    "print(\"# image triples:\", len(tr_train))\n",
    "print(\"# image triples:\", len(tr_val))\n",
    "\n",
    "image_cache = {}\n",
    "\n",
    "tr_load_train = triples_batch(800,tr_train)\n",
    "tr_load_val = triples_batch(106,tr_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 498,
     "status": "ok",
     "timestamp": 1550052298257,
     "user": {
      "displayName": "Luis Caumel",
      "photoUrl": "",
      "userId": "08450739378384452972"
     },
     "user_tz": -60
    },
    "id": "5XMvIFWd7N45",
    "outputId": "9a0e13bd-f0f4-4fe3-d38c-69f833f5d495",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# image train triples: 800\n",
      "# image validation triples: 106\n",
      "(800, 3, 96, 96, 3)\n",
      "(106, 3, 96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"# image train triples:\", len(tr_train))\n",
    "[x for x in tr_train[0:5]]\n",
    "\n",
    "print(\"# image validation triples:\", len(tr_val))\n",
    "[x for x in tr_val[0:5]]\n",
    "\n",
    "print(tr_load_train.shape)\n",
    "\n",
    "print(tr_load_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3908,
     "status": "ok",
     "timestamp": 1550052305912,
     "user": {
      "displayName": "Luis Caumel",
      "photoUrl": "",
      "userId": "08450739378384452972"
     },
     "user_tz": -60
    },
    "id": "Tk9pKrRhq8xL",
    "outputId": "2b360ebc-1ccf-4c67-edb6-c2b6caed1f99"
   },
   "outputs": [],
   "source": [
    "# network definition\n",
    "base_network = create_base_network(input_dim, classes)\n",
    "#base_network.summary()\n",
    "\n",
    "\n",
    "anchor_a = Input(shape=(input_dim))   #Nos da un tensor del tamaño input_dim\n",
    "anchor_p = Input(shape=(input_dim))\n",
    "anchor_n = Input(shape=(input_dim))\n",
    "\n",
    "\n",
    "processed_a = base_network(anchor_a)\n",
    "processed_p = base_network(anchor_p)\n",
    "processed_n = base_network(anchor_n)\n",
    "\n",
    "distance = Lambda(distance_vec, output_shape=abs_diff_output_shape)([processed_a, processed_p, processed_n])\n",
    "\n",
    "pred = Dense(1, activation = 'sigmoid')(distance)\n",
    "\n",
    "model = Model(inputs=[anchor_a, anchor_p, anchor_n], outputs=pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(data,dim):\n",
    "  data_a = data[:, 0].reshape(-1, dim[0], dim[1], dim[2])\n",
    "  data_p = data[:, 1].reshape(-1, dim[0], dim[1], dim[2])\n",
    "  data_n = data[:, 2].reshape(-1, dim[0], dim[1], dim[2])\n",
    "  return [data_a,data_p,data_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2nDUFYuEAB9u",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 96, 96, 3)\n",
      "(800, 96, 96, 3)\n",
      "(800, 96, 96, 3)\n",
      "(106, 96, 96, 3)\n",
      "(106, 96, 96, 3)\n",
      "(106, 96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "#Datos de entrenamiento\n",
    "\n",
    "tr_load_train_ = reshape(tr_load_train,input_dim)\n",
    "tr_load_train_a = tr_load_train_[0]\n",
    "tr_load_train_p = tr_load_train_[1]\n",
    "tr_load_train_n = tr_load_train_[2]\n",
    "\n",
    "print(tr_load_train_a.shape)\n",
    "print(tr_load_train_p.shape)\n",
    "print(tr_load_train_n.shape)\n",
    "\n",
    "#Datos de validacion\n",
    "\n",
    "tr_load_val_ = reshape(tr_load_val,input_dim)\n",
    "tr_load_val_a = tr_load_val_[0]\n",
    "tr_load_val_p = tr_load_val_[1]\n",
    "tr_load_val_n = tr_load_val_[2]\n",
    "\n",
    "print(tr_load_val_a.shape)\n",
    "print(tr_load_val_p.shape)\n",
    "print(tr_load_val_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 475,
     "status": "ok",
     "timestamp": 1550052350856,
     "user": {
      "displayName": "Luis Caumel",
      "photoUrl": "",
      "userId": "08450739378384452972"
     },
     "user_tz": -60
    },
    "id": "str8zx0HBFXP",
    "outputId": "e43ddd6e-351c-438f-d702-71d3a8dc48a2",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 96, 96, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 96, 96, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 96, 96, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 1024)         8673152     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1024)         0           sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "                                                                 sequential_1[3][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            1025        lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 8,674,177\n",
      "Trainable params: 8,671,297\n",
      "Non-trainable params: 2,880\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "INIT_LR = 1e-3\n",
    "EPOCHS = 100\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=triple_loss, optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gAikSlsZEBOI",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 106 samples\n",
      "Epoch 1/10\n",
      "800/800 [==============================] - 45s 56ms/step - loss: 0.1605 - acc: 0.5525 - val_loss: 0.3924 - val_acc: 0.2547\n",
      "Epoch 2/10\n",
      "768/800 [===========================>..] - ETA: 1s - loss: 0.1753 - acc: 0.6589"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-643f3caa8dec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_load_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_load_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr_load_train_a\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtr_load_train_a\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtr_load_train_a\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr_load_val_a\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtr_load_val_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtr_load_val_n\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    210\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[1;32m    211\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                                              verbose=0)\n\u001b[0m\u001b[1;32m    213\u001b[0m                         \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                         \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = np.ones(len(tr_load_train))\n",
    "val = np.ones(len(tr_load_val))\n",
    "a = model.fit([tr_load_train_a,tr_load_train_a,tr_load_train_a], train, batch_size=batch, epochs=nb_epoch, validation_data=([tr_load_val_a,tr_load_val_p,tr_load_val_n], val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,(int)(40/number_images)):\n",
    "    triples_data = create_triplet(number_images,i*number_images,i*number_images+number_images-1)\n",
    "\n",
    "    print(\"# image triples:\", len(triples_data))\n",
    "\n",
    "    tm_train = 800\n",
    "    tm_val = 906\n",
    "\n",
    "    tr_train = triples_data[0:tm_train]\n",
    "    tr_val = triples_data[tm_train:tm_val]  #El tamaño tiene que ser par (No se mucho por que)\n",
    "\n",
    "    print(\"# image triples:\", len(tr_train))\n",
    "    print(\"# image triples:\", len(tr_val))\n",
    "\n",
    "    tr_load_train = triples_batch(800,tr_train)\n",
    "    tr_load_val = triples_batch(106,tr_val)\n",
    "\n",
    "    #Datos de entrenamiento\n",
    "\n",
    "    tr_load_train_ = reshape(tr_load_train,input_dim)\n",
    "    tr_load_train_a = tr_load_train_[0]\n",
    "    tr_load_train_p = tr_load_train_[1]\n",
    "    tr_load_train_n = tr_load_train_[2]\n",
    "\n",
    "    #Datos de validacion\n",
    "\n",
    "    tr_load_val_ = reshape(tr_load_val,input_dim)\n",
    "    tr_load_val_a = tr_load_val_[0]\n",
    "    tr_load_val_p = tr_load_val_[1]\n",
    "    tr_load_val_n = tr_load_val_[2]\n",
    "\n",
    "    train = np.ones(len(tr_load_train))\n",
    "    val = np.ones(len(tr_load_val))\n",
    "    a = model.fit([tr_load_train_a,tr_load_train_a,tr_load_train_a], train, batch_size=batch, epochs=nb_epoch, validation_data=([tr_load_val_a,tr_load_val_p,tr_load_val_n], val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.save('my_model.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "Q9gjsvw88430",
    "vDF1KvPet_Gy"
   ],
   "name": "TFG Software.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
