{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yzuyLp4HCUkX"
   },
   "source": [
    "# **Redes neuronales siamesas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8qNmHTqCUy2"
   },
   "source": [
    "## **Preparacion codigo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V1CDM5EPB4lU"
   },
   "source": [
    "### **Importando modulos necesarios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1727,
     "status": "ok",
     "timestamp": 1550100164594,
     "user": {
      "displayName": "Luis Caumel",
      "photoUrl": "",
      "userId": "08450739378384452972"
     },
     "user_tz": -60
    },
    "id": "5UhweCKcBoOo",
    "outputId": "944cd4bb-c8af-47b9-aeb4-b24f41019cf9"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import,division,print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from PIL import Image as img\n",
    "import cv2\n",
    "from keras.optimizers import Adam\n",
    "import unittest\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Lambda, Flatten, Activation\n",
    "from keras import backend as K\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Datos generales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 471,
     "status": "ok",
     "timestamp": 1550100187214,
     "user": {
      "displayName": "Luis Caumel",
      "photoUrl": "",
      "userId": "08450739378384452972"
     },
     "user_tz": -60
    },
    "id": "DbZ40Cjvp5HQ",
    "outputId": "ae4cc33e-7155-40a9-d5e8-6b8abc80d469"
   },
   "outputs": [],
   "source": [
    "number_images = 40 # Numero de imagenes\n",
    "classes = 5 # Numero de pokemones\n",
    "batch = 32\n",
    "nb_epoch = 2\n",
    "teta = [0.00,0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.10]\n",
    "dataset = 'dataset'\n",
    "database = 'database3'\n",
    "\n",
    "input_dim = (96,96,3) # Son el ancho el alto y la profundidad de la imagen, 3 al ser en color es la variable input_dim pero todavia no esta tocada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EYEWUpcrG3Vv"
   },
   "source": [
    "### **Funcion para calcular la distantica L1 punto a punto entre dos vectores y triplet loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aH1ZRvQPDtCc"
   },
   "outputs": [],
   "source": [
    "def abs_diff_output_shape(input_shapes):\n",
    "    shape1, shape2, shape3= input_shapes\n",
    "    return shape1\n",
    "\n",
    "def distance_vec(y_pred): #Cosine distance\n",
    "    x = y_pred[0]\n",
    "    y = y_pred[1]\n",
    "    z = y_pred[2]\n",
    "    negative = K.prod(K.stack([x, y], axis=1), axis=1)\n",
    "    return negative\n",
    "\n",
    "def get_abs_diff(y_pred):\n",
    "    x = y_pred[0]\n",
    "    y = y_pred[1]\n",
    "    z = y_pred[2]\n",
    "    return K.abs(x - y)\n",
    "\n",
    "def triple_loss(y_true, y_pred):   # y_pred contiene la imagen ancla, positiva y negativa\n",
    "    alpha = 0.1\n",
    "  \n",
    "    anchor = y_pred[0]\n",
    "    positive = y_pred[1]\n",
    "    negative = y_pred[2]\n",
    "    \n",
    "    positive_distance = K.mean(K.square(anchor-positive),axis=-1)  #En principio mean y sum hacen lo mismo excepto que una hace media y otro suma\n",
    "    negative_distance = K.mean(K.square(anchor-negative),axis=-1)\n",
    "  \n",
    "    loss = positive_distance - negative_distance\n",
    "    return K.maximum(loss + alpha, 0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TestSum(unittest.TestCase):\n",
    "\n",
    "    def WhenIWantToGetAnImageThisMustBeRandom(self):\n",
    "        img_groups = {'001' : '001'}\n",
    "        group_names = list(img_groups.keys())\n",
    "        image = get_random_image(img_groups,group_names,int('001'))\n",
    "        self.asserEqual(image,'001001.jpg')\n",
    "        \n",
    "    def WhenIWantToCreateTripletThisMustBeCreated(self):\n",
    "        array1 = create_triplet(1)\n",
    "        array2 = ['001001.jpg','001023.jpg','004003.jpg']\n",
    "        self.asserEqual(array,array2)\n",
    "        \n",
    "    def WhenIWantToLoadAndImageTheImageMustBeLoad(self):\n",
    "        image = cv2.imread('/home/luis/dataset/' + '001' + '/' + '001001.jpg')\n",
    "        image = cv2.resize(image, (96, 96))\n",
    "        image2 = img_to_array(image)\n",
    "        image1 = load_image('001001.jpg')\n",
    "        self.asserEqual(image1,image2)\n",
    "    def WhenIHaveMyTripletsIWantToLoadIt(slef):\n",
    "        triplets = triples_batch(1,image_triples)\n",
    "        images = np.empty([1,3,96,96,3])\n",
    "        for index,i in enumerate(image_triples):\n",
    "            ahs, phs, nhs= i\n",
    "            a = (load_image(ahs),load_image(phs),load_image(nhs))\n",
    "            images[index] = a\n",
    "        self.asserEqual(triplets,images)\n",
    "    def WhenIWantToReshapeMyTripletsThisMustBeReshape(self):\n",
    "        triples_data = create_triplet(1)\n",
    "        image_cache = {}\n",
    "        tr_load_train = triples_batch(tr_train)\n",
    "        tr_load_train_ = reshape(tr_load_train,(96,96,3))\n",
    "        self.asserEqual(tr_load_val_a.shape,(96,96,3))\n",
    "    def WhenIWantToCalculateTheLoss(self):\n",
    "        y_true = 1\n",
    "        triples_data = create_triplet(1)\n",
    "        image_cache = {}\n",
    "        tr_load_train = triples_batch(tr_train)\n",
    "        y_pred = reshape(tr_load_train,(96,96,3))\n",
    "        loss = triple_loss(y_true, y_pred)\n",
    "        self.asserEqual(loss,0)\n",
    "    \n",
    "    def WhenIWantToCalculateTheCosineDistance(self):\n",
    "        y_true = 1\n",
    "        triples_data = create_triplet(1)\n",
    "        image_cache = {}\n",
    "        tr_load_train = triples_batch(tr_train)\n",
    "        y_pred = reshape(tr_load_train,(96,96,3))\n",
    "        distance = distance_vec(y_pred)\n",
    "        self.asserEqual(distance,0)\n",
    "\n",
    "  \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HksRdHOBGsiX"
   },
   "source": [
    "### **Funcion para crear tripletas y cargar las imagenes**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jlov2ltiHFGo"
   },
   "outputs": [],
   "source": [
    "def get_random_image(img_groups, group_names, gid):\n",
    "    gname = group_names[gid]\n",
    "    photos = img_groups[gname]\n",
    "    pid = np.random.choice(np.arange(len(photos)), size=1)[0]\n",
    "    pname = photos[pid]\n",
    "    return gname + pname + \".jpg\"\n",
    "   \n",
    "def create_triplet(number_images,minImage,maxImage): # Crea tripleta\n",
    "    img_groups = {}\n",
    "    for folder in  sorted(os.listdir('/home/luis/' + dataset),reverse=False):\n",
    "        i = 0 \n",
    "        for img_file in sorted(os.listdir('/home/luis/' + dataset + '/' + folder),reverse=False):\n",
    "        #if i == number_images: # Para solo coger 40 fotos\n",
    "            #break\n",
    "            if i < minImage:\n",
    "                i = i + 1\n",
    "                continue\n",
    "            if i > maxImage:\n",
    "                break\n",
    "            prefix, suffix = img_file.split(\".\")\n",
    "            pid = prefix[3:]\n",
    "            if folder in img_groups:\n",
    "                img_groups[folder].append(pid)\n",
    "            else:\n",
    "                img_groups[folder] = [pid]\n",
    "            i += 1\n",
    "    pos_triples, neg_triples, labels = [], [], []\n",
    "  # positive pairs and negative \n",
    "    group_names = list(img_groups.keys())\n",
    "    for key in img_groups.keys():\n",
    "        for i in range(0,number_images):\n",
    "            for j in range(i+1,number_images): # Si dejo i me cogeria duplas con elementos iguales osea (001,001)\n",
    "                inc = random.randrange(1, classes+1)\n",
    "                dn = (int(key) + inc) % classes\n",
    "                right = get_random_image(img_groups, group_names, dn)\n",
    "                pos_triples.append((key + img_groups[key][i] + \".jpg\", key + img_groups[key][j] + \".jpg\", right))\n",
    "    return pos_triples  \n",
    "def load_image(image_name):\n",
    "    image = cv2.imread('/home/luis/' + dataset + '/' + image_name[0:3] + '/' + image_name)\n",
    "    image = cv2.resize(image, (96, 96))\n",
    "    image = img_to_array(image)\n",
    "    image = image / 255.0\n",
    "    return image     \n",
    "           \n",
    "def triples_batch(tamaño,image_triples):#, batch_size): #Tripletas\n",
    "    images = np.empty([tamaño,3,96,96,3],dtype=\"float16\")\n",
    "    for index,i in enumerate(image_triples):\n",
    "        ahs, phs, nhs= i\n",
    "        a = (load_image(ahs),load_image(phs),load_image(nhs))\n",
    "        images[index] = a\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_image2(img_groups, group_names, gid):     #Este metodo es para juntar todas con todas\n",
    "    gname = group_names[gid]\n",
    "    photos = img_groups[gname]\n",
    "    pid = np.random.choice(np.arange(len(photos)), size=1)[0]\n",
    "    pname = photos[pid]\n",
    "    return gname + pname + \".jpg\"\n",
    "   \n",
    "def create_triplet2(number_images,minImage,maxImage): # Crea tripleta\n",
    "    img_groups = {}\n",
    "    for folder in  sorted(os.listdir('/home/luis/dataset'),reverse=False):\n",
    "        i = 0 \n",
    "        for img_file in sorted(os.listdir('/home/luis/dataset/' + folder),reverse=False):\n",
    "          #if i == number_images: # Para solo coger 40 fotos\n",
    "            #break\n",
    "            if i < minImage:\n",
    "                i = i + 1\n",
    "                continue\n",
    "            if i > maxImage:\n",
    "                break\n",
    "            print(img_file)\n",
    "            prefix, suffix = img_file.split(\".\")\n",
    "            pid = prefix[3:]\n",
    "            if folder in img_groups:\n",
    "                 img_groups[folder].append(pid)\n",
    "            else:\n",
    "                 img_groups[folder] = [pid]\n",
    "            i += 1\n",
    "    pos_triples = []\n",
    "      # positive pairs and negative \n",
    "    group_names = list(img_groups.keys())\n",
    "    for key in img_groups.keys():\n",
    "        for i in range(0,number_images):\n",
    "            for j in range(i+1,number_images): # Si dejo i me cogeria duplas con elementos iguales osea (001,001)\n",
    "                for k in img_groups.keys():\n",
    "                    if k != key:\n",
    "                        for l in range(0,number_images):\n",
    "                            print((key + img_groups[key][i] + \".jpg\", key + img_groups[key][j] + \".jpg\", k + img_groups[k][l]))\n",
    "                            pos_triples.append((key + img_groups[key][i] + \".jpg\", key + img_groups[key][j] + \".jpg\", k + img_groups[k][l]))\n",
    "    return pos_triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aniGPta3HVE7"
   },
   "source": [
    "### **Crear red neuronal Convolucional**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KlIPAdDyHRcz"
   },
   "outputs": [],
   "source": [
    "def create_base_network(input_dim, classes): # SmallerVGGnet\n",
    "    red = Sequential() \n",
    "    red.add(Conv2D(32, (3, 3), padding=\"same\",input_shape=input_dim))\n",
    "    red.add(Activation(\"relu\"))\n",
    "    red.add(BatchNormalization(axis=-1))\n",
    "    red.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "    #red.add(Dropout(0.25))\n",
    "\n",
    "    # (CONV => RELU) * 2 => POOL\n",
    "    red.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "    red.add(Activation(\"relu\"))\n",
    "    red.add(BatchNormalization(axis=-1))\n",
    "    red.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "    red.add(Activation(\"relu\"))\n",
    "    red.add(BatchNormalization(axis=-1))\n",
    "    red.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #red.add(Dropout(0.25))\n",
    "\n",
    "    # (CONV => RELU) * 2 => POOL\n",
    "    red.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "    red.add(Activation(\"relu\"))\n",
    "    red.add(BatchNormalization(axis=-1))\n",
    "    red.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "    red.add(Activation(\"relu\"))\n",
    "    red.add(BatchNormalization(axis=-1))\n",
    "    red.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #red.add(Dropout(0.25))\n",
    "\n",
    "    # first (and only) set of FC => RELU layers\n",
    "    red.add(Flatten())\n",
    "    #red.add(Dense(2048))\n",
    "    #red.add(Activation(\"relu\"))\n",
    "    #red.add(BatchNormalization())\n",
    "    #red.add(Dropout(0.5))\n",
    "\n",
    "    red.add(Dense(1024))\n",
    "    red.add(Activation(\"relu\"))\n",
    "    red.add(BatchNormalization())\n",
    "    red.add(Dropout(0.5))\n",
    "\n",
    "    return red "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tVpDskYQqno1"
   },
   "source": [
    "## **Principal**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0EePEpaNqw4a"
   },
   "source": [
    "### **Inicializar datos y normalizarlos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vDF1KvPet_Gy"
   },
   "source": [
    "### Crear varias imagenes apartir de una moviendola o rotandola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FXrC8WJ_kmdZ"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '/home/luis/dataset',  # this is the target directory\n",
    "        target_size=(256, 256),  # all images will be resized to 150x150\n",
    "        batch_size=16,\n",
    "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "#(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train = X_train.reshape(60000, input_dim)\n",
    "#X_test = X_test.reshape(10000, input_dim)\n",
    "#X_train = X_train.astype('float32')\n",
    "#X_test = X_test.astype('float32')\n",
    "#X_train /= 255\n",
    "#X_test /= 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_t5-FJBqAMK"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "img = load_img('/home/luis/database/108.png')  # this is a PIL image\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1,save_to_dir='/home/luis/', save_prefix='1', save_format='png'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break  # otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V9SCO5vqq78g"
   },
   "source": [
    "### **Crear conjuntos de datos de entrenamiento y test** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGB199xDq8bn",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "triples_data = create_triplet(number_images,0,number_images)\n",
    "\n",
    "print(\"# image triples:\", len(triples_data))\n",
    "\n",
    "tm_train = 800\n",
    "tm_val = 906\n",
    "\n",
    "tr_train = triples_data[0:tm_train]\n",
    "tr_val = triples_data[tm_train:tm_val]  #El tamaño tiene que ser par (No se mucho por que)\n",
    "\n",
    "print(\"# image triples:\", len(tr_train))\n",
    "print(\"# image triples:\", len(tr_val))\n",
    "\n",
    "image_cache = {}\n",
    "\n",
    "tr_load_train = triples_batch(800,tr_train)\n",
    "tr_load_val = triples_batch(106,tr_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 498,
     "status": "ok",
     "timestamp": 1550052298257,
     "user": {
      "displayName": "Luis Caumel",
      "photoUrl": "",
      "userId": "08450739378384452972"
     },
     "user_tz": -60
    },
    "id": "5XMvIFWd7N45",
    "outputId": "9a0e13bd-f0f4-4fe3-d38c-69f833f5d495",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"# image train triples:\", len(tr_train))\n",
    "[x for x in tr_train[0:5]]\n",
    "\n",
    "print(\"# image validation triples:\", len(tr_val))\n",
    "[x for x in tr_val[0:5]]\n",
    "\n",
    "print(tr_load_train.shape)\n",
    "\n",
    "print(tr_load_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ya7x2IZ4q8mN"
   },
   "source": [
    "### **Crear red siamesa** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3908,
     "status": "ok",
     "timestamp": 1550052305912,
     "user": {
      "displayName": "Luis Caumel",
      "photoUrl": "",
      "userId": "08450739378384452972"
     },
     "user_tz": -60
    },
    "id": "Tk9pKrRhq8xL",
    "outputId": "2b360ebc-1ccf-4c67-edb6-c2b6caed1f99"
   },
   "outputs": [],
   "source": [
    "# network definition\n",
    "base_network = create_base_network(input_dim, classes)\n",
    "#base_network.summary()\n",
    "\n",
    "\n",
    "anchor_a = Input(shape=(input_dim))   #Nos da un tensor del tamaño input_dim\n",
    "anchor_p = Input(shape=(input_dim))\n",
    "anchor_n = Input(shape=(input_dim))\n",
    "\n",
    "\n",
    "processed_a = base_network(anchor_a)\n",
    "processed_p = base_network(anchor_p)\n",
    "processed_n = base_network(anchor_n)\n",
    "\n",
    "distance = Lambda(distance_vec, output_shape=abs_diff_output_shape)([processed_a, processed_p, processed_n])\n",
    "\n",
    "pred = Dense(1, activation = 'sigmoid')(distance)\n",
    "\n",
    "model = Model(inputs=[anchor_a, anchor_p, anchor_n], outputs=pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dlSqy90sq89q"
   },
   "source": [
    "### **Entrenar modelo**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(data,dim):\n",
    "    data_a = data[:, 0].reshape(-1, dim[0], dim[1], dim[2])\n",
    "    data_p = data[:, 1].reshape(-1, dim[0], dim[1], dim[2])\n",
    "    data_n = data[:, 2].reshape(-1, dim[0], dim[1], dim[2])\n",
    "    return [data_a,data_p,data_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2nDUFYuEAB9u",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Datos de entrenamiento\n",
    "\n",
    "tr_load_train_ = reshape(tr_load_train,input_dim)\n",
    "tr_load_train_a = tr_load_train_[0]\n",
    "tr_load_train_p = tr_load_train_[1]\n",
    "tr_load_train_n = tr_load_train_[2]\n",
    "\n",
    "print(tr_load_train_a.shape)\n",
    "print(tr_load_train_p.shape)\n",
    "print(tr_load_train_n.shape)\n",
    "\n",
    "#Datos de validacion\n",
    "\n",
    "tr_load_val_ = reshape(tr_load_val,input_dim)\n",
    "tr_load_val_a = tr_load_val_[0]\n",
    "tr_load_val_p = tr_load_val_[1]\n",
    "tr_load_val_n = tr_load_val_[2]\n",
    "\n",
    "print(tr_load_val_a.shape)\n",
    "print(tr_load_val_p.shape)\n",
    "print(tr_load_val_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 475,
     "status": "ok",
     "timestamp": 1550052350856,
     "user": {
      "displayName": "Luis Caumel",
      "photoUrl": "",
      "userId": "08450739378384452972"
     },
     "user_tz": -60
    },
    "id": "str8zx0HBFXP",
    "outputId": "e43ddd6e-351c-438f-d702-71d3a8dc48a2"
   },
   "outputs": [],
   "source": [
    "INIT_LR = 1e-3\n",
    "EPOCHS = 100\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=triple_loss, optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gAikSlsZEBOI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = np.ones(len(tr_load_train))\n",
    "val = np.ones(len(tr_load_val))\n",
    "a = model.fit([tr_load_train_a,tr_load_train_p,tr_load_train_n], train, batch_size=batch, epochs=nb_epoch, validation_data=([tr_load_val_a,tr_load_val_p,tr_load_val_n], val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1500,
     "status": "ok",
     "timestamp": 1550080484838,
     "user": {
      "displayName": "Luis Caumel",
      "photoUrl": "",
      "userId": "08450739378384452972"
     },
     "user_tz": -60
    },
    "id": "JLhDWbl_T36_",
    "outputId": "d43db674-e4fd-4167-87d8-d8893445631b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(a.history[\"loss\"], color=\"r\", label=\"train\")\n",
    "plt.plot(a.history[\"val_loss\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(a.history[\"acc\"], color=\"r\", label=\"train\")\n",
    "plt.plot(a.history[\"val_acc\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,(int)(40/number_images)):\n",
    "    print((str)(i*number_images) + ' '+ (str)(i*number_images+number_images-1))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,(int)(40/number_images)):\n",
    "    del triples_data\n",
    "    del tr_load_train\n",
    "    del tr_load_val\n",
    "    triples_data = create_triplet(number_images,i*number_images,i*number_images+number_images-1)\n",
    "\n",
    "    print(\"# image triples:\", len(triples_data))\n",
    "   \n",
    "    tm_train = 800\n",
    "    tm_val = 906\n",
    "\n",
    "    tr_train = triples_data[0:tm_train]\n",
    "    tr_val = triples_data[tm_train:tm_val]  #El tamaño tiene que ser par (No se mucho por que)\n",
    "\n",
    "    print(\"# image triples:\", len(tr_train))\n",
    "    print(\"# image triples:\", len(tr_val))\n",
    "    \n",
    "    tr_load_train = triples_batch(800,tr_train)\n",
    "    tr_load_val = triples_batch(106,tr_val)\n",
    "\n",
    "    #Datos de entrenamiento\n",
    "\n",
    "    tr_load_train_ = reshape(tr_load_train,input_dim)\n",
    "    tr_load_train_a = tr_load_train_[0]\n",
    "    tr_load_train_p = tr_load_train_[1]\n",
    "    tr_load_train_n = tr_load_train_[2]\n",
    "\n",
    "    #Datos de validacion\n",
    "\n",
    "    tr_load_val_ = reshape(tr_load_val,input_dim)\n",
    "    tr_load_val_a = tr_load_val_[0]\n",
    "    tr_load_val_p = tr_load_val_[1]\n",
    "    tr_load_val_n = tr_load_val_[2]\n",
    "\n",
    "    train = np.ones(len(tr_load_train))\n",
    "    val = np.ones(len(tr_load_val))\n",
    "    a = model.fit([tr_load_train_a,tr_load_train_p,tr_load_train_n], train, batch_size=batch, epochs=nb_epoch, validation_data=([tr_load_val_a,tr_load_val_p,tr_load_val_n], val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(a.history[\"loss\"], color=\"r\", label=\"train\")\n",
    "plt.plot(a.history[\"val_loss\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(a.history[\"acc\"], color=\"r\", label=\"train\")\n",
    "plt.plot(a.history[\"val_acc\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.save('my_model.h5')\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "paZXfyDSr0VW"
   },
   "source": [
    "# **Test y prueba en funcionamiento**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JaGMjjXCr6W4"
   },
   "source": [
    "## **Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = load_model('my_model.h5', custom_objects={'triple_loss': triple_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t07kR9viKzMe"
   },
   "outputs": [],
   "source": [
    "def load_data(name, file):\n",
    "    if file == 1:\n",
    "        image = cv2.imread('/home/luis/' + database + '/' + name + '.jpg')\n",
    "    else:\n",
    "        image = cv2.imread('/home/luis/' + name + '.jpg')\n",
    "    image = cv2.resize(image, (96, 96))\n",
    "    image = img_to_array(image)\n",
    "    return image  \n",
    "\n",
    "def evaluate_data(name):\n",
    "    values = []\n",
    "    for i in range(1,classes+1):\n",
    "        images = []\n",
    "        name1 = str(i)\n",
    "        len_name = len(name1)\n",
    "        if len_name == 1:\n",
    "            name1= '00' + name1\n",
    "        elif len_name == 2:\n",
    "            name1= '0' + name1\n",
    "        #print(name + ' ' + name1 + ' ' + name1)\n",
    "        images.append((load_data(name, 0),load_data(name1, 1),load_data(name1, 1)))\n",
    "        data = np.array(images, dtype=\"float\") / 255.0\n",
    "        data = reshape(data,input_dim)\n",
    "        a = model.predict([data[0],data[1],data[2]])\n",
    "        #print(a)\n",
    "        values.append(a)\n",
    "    return values\n",
    "\n",
    "def draw_image(subplot, image, title):\n",
    "    plt.subplot(subplot)\n",
    "    plt.imshow(image)\n",
    "    plt.title(title)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "def compare(array1,array2):\n",
    "    a = 0\n",
    "    for i in range(0,(len(array1))):\n",
    "        if array1[i] == array2[i]:\n",
    "            a = a + 1\n",
    "    return a\n",
    "\n",
    "def confusion(imagenes,truePhotos,testPhotos,teta,distancePhotos):\n",
    "    matrix = {}\n",
    "    for l in teta:\n",
    "        matrix.update({l:{}})\n",
    "        for img_file in sorted(os.listdir('/home/luis/' + database + '/'),reverse=False):  #Tomo el nombre de las imagenes de la database para hacer el diccionario\n",
    "            matrix[l].update({img_file[0:3]:[0,0,0,0]})\n",
    "    for k in teta:\n",
    "        for i in range(0,classes):\n",
    "            for j in range(0,len(testPhotos)):\n",
    "                if truePhotos[j] == (i+1) and testPhotos[j] == (i+1) and distancePhotos[j]<=k: #TP\n",
    "                    matrix[k]['00'+ (str)(i+1)][0] += 1\n",
    "                elif truePhotos[j] != (i+1) and testPhotos[j] == (i+1): #FP\n",
    "                    matrix[k]['00'+ (str)(i+1)][1] += 1\n",
    "                elif truePhotos[j] != (i+1) and testPhotos[j] != (i+1): #FN\n",
    "                    matrix[k]['00'+ (str)(i+1)][2] += 1\n",
    "                elif (truePhotos[j] == (i+1) and testPhotos[j] != (i+1)) or (truePhotos[j] == (i+1) and testPhotos[j] == (i+1) and distancePhotos[j]>=k): #TN\n",
    "                    matrix[k]['00'+ (str)(i+1)][3] += 1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenes = ['001','002','003','004','005'] #Nombre imagenes de test\n",
    "truePhotos = [1,3,4,4,1]  \n",
    "testPhotos = [1,2,3,4,5]\n",
    "distancePhotos = [0.1,0.2,0.3,0.4,0.5]\n",
    "\n",
    "matrix = confusion(imagenes,truePhotos,testPhotos,teta,distancePhotos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in matrix:\n",
    "    print(i,matrix[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Test con una imagen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = evaluate_data('003')\n",
    "for index,i in enumerate(x):\n",
    "    print(index+1,i)\n",
    "var = 1000\n",
    "index_var = 0\n",
    "for index,j in enumerate(x):\n",
    "     if j <= var:\n",
    "        var = j\n",
    "        index_var=index\n",
    "print('Tu pokemon es el',index_var+1,var)\n",
    "ref_image = plt.imread(os.path.join('/home/luis/' + database + '/' + '00' +(str)(index_var+1) + '.jpg'))\n",
    "draw_image(131,ref_image, \"Pokemon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            ['001','002','003','004','005','006','007','008','009',\n",
    "            '010','011','012','013','014','015','016','017','018','019',\n",
    "            '020','021','022','023','024','025','026','027','028','029',\n",
    "            '030','031','032','033','034','035','036','037','038','039',\n",
    "            '040','041','042','043','044','045','046','047','048','049',\n",
    "            '050','051','052','053','054','055','056','057','058','059',\n",
    "            '060','061','062','063','064','065','066','067','068','069',\n",
    "            '070','071','072','073','074','075','076','077','078','079',\n",
    "            '080','081','082','083','084','085','086','087','088','089',\n",
    "            '090','091','092','093','094','095','096','097','098','099',\n",
    "            '100','101','102','103','104','105','106','107','108','109',\n",
    "            '110','111','112','113','114','115','116','117','118','119',\n",
    "            '120','121','122','123','124','125','126','127','128','129',\n",
    "            '130','131','132','133','134','135','136','137','138','139',\n",
    "            '140','141','142','143','144','145','146','147','148','149',\n",
    "            '150','151']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Test con un conjunto de imagenes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "76juyf6xYvh9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imagenes = ['001','002','003','004','005','006','007','008','009',\n",
    "            '010','011','012','013','014','015','016','017','018','019',\n",
    "            '020','021','022','023','024','025','026','027','028','029',\n",
    "            '030','031','032','033','034','035']\n",
    "images = [1,1,2,2,3,3,4,4,5,5,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,5,5,5,5,5]\n",
    "\n",
    "values = []\n",
    "for i in range(0,len(images)):\n",
    "    x = evaluate_data(imagenes[i])\n",
    "    values.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 520,
     "status": "ok",
     "timestamp": 1550081622680,
     "user": {
      "displayName": "Luis Caumel",
      "photoUrl": "",
      "userId": "08450739378384452972"
     },
     "user_tz": -60
    },
    "id": "j_-e1d8lsDtO",
    "outputId": "12e5212c-a5a6-4925-fe0c-9a4da243d81e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aciertos = []\n",
    "distancia = []\n",
    "for i in values:\n",
    "    var = 1000\n",
    "    index_var = 0\n",
    "    for index,j in enumerate(i):\n",
    "        if j <= var:\n",
    "            var = j\n",
    "            index_var=index\n",
    "    print(index_var+1,var)\n",
    "    aciertos.append(index_var+1)\n",
    "    distancia.append(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 520,
     "status": "ok",
     "timestamp": 1550081622680,
     "user": {
      "displayName": "Luis Caumel",
      "photoUrl": "",
      "userId": "08450739378384452972"
     },
     "user_tz": -60
    },
    "id": "j_-e1d8lsDtO",
    "outputId": "12e5212c-a5a6-4925-fe0c-9a4da243d81e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Accuracy y Matriz de confusión\n",
    "imagenes = list(map(int, imagenes))\n",
    "accuracy = compare(aciertos, images)\n",
    "matrix = confusion(imagenes,images,aciertos,teta,distancia)\n",
    "print()\n",
    "print('El % de aciertos es: ', (accuracy/35)*100, '%')\n",
    "print()\n",
    "ROCsx = {}\n",
    "ROCsy = {}\n",
    "for img_file in sorted(os.listdir('/home/luis/' + database + '/'),reverse=False):  #Tomo el nombre de las imagenes de la database para hacer el diccionario\n",
    "    ROCsx.update({img_file[0:3]:[]})\n",
    "    ROCsy.update({img_file[0:3]:[]})\n",
    "for i in matrix:\n",
    "    print('##############################################################################')\n",
    "    print()\n",
    "    print('Con teta: ',i)\n",
    "    print()\n",
    "    for j in matrix[i]:\n",
    "        print('##############################################################################')\n",
    "        print()\n",
    "        print('La matrix de confusión para el pokemon ' + j + ' con teta',i, ' es: ' )\n",
    "        print('\\n Verdadero positivo: ',matrix[i][j][0])\n",
    "        print('\\n Falso positivo: ',matrix[i][j][1])\n",
    "        print('\\n Falso negativo: ',matrix[i][j][2])\n",
    "        print('\\n Verdadero negativo: ',matrix[i][j][3])\n",
    "        print()\n",
    "        print('La exactitud de predicción en el pokemon ',j,' es: ', ((matrix[i][j][0]+matrix[i][j][3])/(matrix[i][j][0]+matrix[i][j][3]+matrix[i][j][1]+matrix[i][j][2])))\n",
    "        TPR = matrix[i][j][0]/(matrix[i][j][0]+matrix[i][j][2])\n",
    "        FPR = matrix[i][j][1]/(matrix[i][j][1]+matrix[i][j][3])\n",
    "        print('La TPR del pokemon ',j,' es: ',TPR)\n",
    "        print('La FPR del pokemon ',j,' es: ',FPR)\n",
    "        ROCsy[j].append(TPR)\n",
    "        ROCsx[j].append(FPR)\n",
    "        print()\n",
    "print('##############################################################################')\n",
    "print()\n",
    "print('Curva ROC')\n",
    "for img_file in sorted(os.listdir('/home/luis/' + database + '/'),reverse=False):  #Tomo el nombre de las imagenes de la database para hacer el diccionario\n",
    "    print('Para el pokemon: ', img_file[0:3])\n",
    "    plt.plot(ROCsx[img_file[0:3]],ROCsy[img_file[0:3]],'-o')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "Q9gjsvw88430",
    "vDF1KvPet_Gy"
   ],
   "name": "TFG Software.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
